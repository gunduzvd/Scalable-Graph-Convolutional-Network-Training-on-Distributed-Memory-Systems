# Data Preprocessing

Before running partitioning codes, data preprocessing is applied to normalize the adjacency matrix and to produce config files indicating how many features and layers are needed.

python GrB-GNN-IDG.py -i {PATH-TO-FILE} -f {nfeatures} -l {nlayers}

{PATH-TO-FILE} should point to a graph in matrix market format. Datasets in the paper are available through https://sparse.tamu.edu/

-f {nfeatures} : number of features per layer

-l {nfeatures} : number of layers

# Graph(GCN-GP) and Hypergraph(GCN-HP) Partitioning Codes

The input matrix partitioning code for parallel GCN training algorithm. The code uses patoh and metis partitioning libraries.
Modify INC_DIR and LIB_DIR to point appropriate locations in makefile.

To compile the partitioning code just use the command:

make

To run the executable run:

Hypergraph Partitioning:

./gcnhgp -a {Par_A} -h {Par_H} -o {Par_O} -k {k} -f {nfeatures} -l {nlayers}  

Graph Partitioning:

./gcngp -a {Par_A} -h {Par_H} -o {Par_O} -k {k} -f {nfeatures} -l {nlayers} 

Parameters:

-a {Par_A} : path to adjacecny matrix 

-h {Par_H} : path to input vertex features 

-o {Par_O} : output folder for partitioned matrices 

-k {k} : Number of partitions 

-f {nfeatures} : number of features per layer 

-l {nlayers} : number of layers 

# Scalable Graph Convolutional Network Training on Distributed-Memory Systems

The code requires SuiteSparse:GraphBLAS (https://github.com/DrTimothyAldenDavis/GraphBLAS) and MPI libraries. Modify INC_DIR and LIB_DIR variables in the makefile to point correct locations.

To compile the code just run the command:

make

To run the executable:

mpirun -n {PATH_TO_PARTS} -c {NTHREADS}

Variables {PATH_TO_CONFIG} must store the path to the directory storing partitioned input matrices and the path to the configuration file, respectively. Directory and configuratin file are generated by the partitioning code.



